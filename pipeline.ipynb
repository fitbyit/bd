{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, Imputer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Download CSV\n",
        "!wget https://raw.githubusercontent.com/neelamdoshi/Spark_neelam/main/diabetes.csv\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DiabetesLogisticRegressionWithImputer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load and inspect the data\n",
        "data = spark.read.csv(\"diabetes.csv\", header=True, inferSchema=True)\n",
        "data.printSchema()\n",
        "data.show(5)\n",
        "\n",
        "# Step 1: Replace zero values with nulls in specific columns where zero is invalid\n",
        "columns_with_zero_as_missing = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
        "\n",
        "# Replace zero values with null in specified columns\n",
        "for column in columns_with_zero_as_missing:\n",
        "    data = data.withColumn(column, when(col(column) == 0, None).otherwise(col(column)))\n",
        "\n",
        "# Step 2: Impute missing (null) values using the median strategy\n",
        "feature_cols = [col for col in data.columns if col != 'Outcome']\n",
        "imputer = Imputer(inputCols=feature_cols, outputCols=[f\"{col}_imputed\" for col in feature_cols])\\\n",
        "    .setStrategy(\"median\")  # Set strategy to median\n",
        "\n",
        "# Apply the Imputer\n",
        "imputed_data = imputer.fit(data).transform(data)\n",
        "imputed_data.show(5)  # Inspect imputed data\n",
        "\n",
        "# Step 3: Feature assembly and scaling\n",
        "# Use imputed columns for feature assembling\n",
        "imputed_feature_cols = [f\"{col}_imputed\" for col in feature_cols]\n",
        "\n",
        "# Assemble the feature columns into a single vector\n",
        "assembler = VectorAssembler(inputCols=imputed_feature_cols, outputCol=\"features\")\n",
        "\n",
        "# Standardize the feature vectors (scaling)\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "# Create the label column from Outcome (cast it to double type)\n",
        "imputed_data = imputed_data.withColumn(\"label\", col(\"Outcome\").cast(\"double\"))\n",
        "\n",
        "# Step 4: Create a Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"label\")\n",
        "\n",
        "# Step 5: Create a Pipeline with stages: Imputer -> Assembler -> Scaler -> Logistic Regression\n",
        "pipeline = Pipeline(stages=[imputer, assembler, scaler, lr])\n",
        "\n",
        "# Step 6: Train-test split\n",
        "train_data, test_data = imputed_data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Step 7: Train the model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# Step 8: Make predictions on the test data\n",
        "predictions = model.transform(test_data)\n",
        "predictions.select(\"label\", \"prediction\", \"probability\").show(5)\n",
        "\n",
        "# Step 9: Evaluate the model using BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "dpRmJRjmfpPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}